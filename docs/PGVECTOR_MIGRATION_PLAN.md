# Plano de Migra√ß√£o: pgvector com HNSW

**Status:** N√ÉO IMPLEMENTADO (n√£o necess√°rio para 276 registros)  
**Trigger de Implementa√ß√£o:** Dataset > 5.000 registros OU busca > 500ms

## Performance Atual vs Esperada com HNSW

| Dataset Size | Scan Completo (atual) | HNSW (pgvector) | Ganho |
|--------------|----------------------|-----------------|-------|
| 276          | ~150ms ‚úÖ            | ~80ms           | 1.9x  |
| 1.000        | ~350ms               | ~60ms           | 5.8x  |
| 10.000       | ~900ms ‚ö†Ô∏è            | ~40ms ‚úÖ        | 22.5x |
| 100.000      | ~8s ‚ùå               | ~50ms ‚úÖ        | 160x  |

## Quando Implementar?

### ‚úÖ Triggers de Migra√ß√£o:
- Dataset ultrapassar 5.000 registros
- Tempo m√©dio de busca > 500ms
- Planos de crescimento r√°pido (>1.000 registros/m√™s)

### ‚ùå N√£o implementar se:
- Dataset < 5.000 registros
- Performance atual satisfat√≥ria (<300ms)
- Precis√£o de 100% √© requisito cr√≠tico

## Passo a Passo da Migra√ß√£o

### 1. Instalar extens√£o pgvector

```powershell
# Baixar e instalar pgvector no PostgreSQL 15
# https://github.com/pgvector/pgvector

# No banco:
$env:PGPASSWORD = "rx1800"
& "C:\Program Files\PostgreSQL\15\bin\psql.exe" `
  -h localhost -p 5433 -U postgres -d hub_aura_db `
  -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

### 2. Criar migration Alembic

```python
# migrations/versions/YYYYMMDD_migrate_to_pgvector.py

"""Migra√ß√£o para pgvector com √≠ndice HNSW

Revision ID: migrate_to_pgvector
Revises: 20251030_add_objeto_vetor_v3
Create Date: YYYY-MM-DD

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import ARRAY
from pgvector.sqlalchemy import Vector

revision = 'migrate_to_pgvector'
down_revision = '20251030_add_objeto_vetor_v3'

def upgrade():
    # Criar extens√£o vector
    op.execute("CREATE EXTENSION IF NOT EXISTS vector;")
    
    # Criar novas colunas do tipo vector
    op.execute("""
        ALTER TABLE documento_vetores 
        ADD COLUMN IF NOT EXISTS objeto_vetor_v2_vector vector(384);
    """)
    
    op.execute("""
        ALTER TABLE documento_vetores 
        ADD COLUMN IF NOT EXISTS objeto_vetor_v3_vector vector(384);
    """)
    
    # Copiar dados de FLOAT[] para vector
    op.execute("""
        UPDATE documento_vetores 
        SET objeto_vetor_v2_vector = objeto_vetor_v2::vector
        WHERE objeto_vetor_v2 IS NOT NULL;
    """)
    
    op.execute("""
        UPDATE documento_vetores 
        SET objeto_vetor_v3_vector = objeto_vetor_v3::vector
        WHERE objeto_vetor_v3 IS NOT NULL;
    """)
    
    # Criar √≠ndices HNSW
    # m=16: n√∫mero de conex√µes por n√≥ (padr√£o: 16, range: 4-64)
    # ef_construction=64: tamanho da fila durante constru√ß√£o (padr√£o: 64, range: 10-200)
    op.execute("""
        CREATE INDEX idx_objeto_vetor_v2_hnsw 
        ON documento_vetores 
        USING hnsw (objeto_vetor_v2_vector vector_cosine_ops)
        WITH (m = 16, ef_construction = 64);
    """)
    
    op.execute("""
        CREATE INDEX idx_objeto_vetor_v3_hnsw 
        ON documento_vetores 
        USING hnsw (objeto_vetor_v3_vector vector_cosine_ops)
        WITH (m = 16, ef_construction = 64);
    """)
    
    print("‚úÖ Migra√ß√£o para pgvector conclu√≠da!")
    print("‚ö†Ô∏è  Colunas antigas (FLOAT[]) mantidas para rollback")
    print("üìä Pr√≥ximo passo: atualizar main.py para usar vector")

def downgrade():
    # Remover √≠ndices
    op.execute("DROP INDEX IF EXISTS idx_objeto_vetor_v3_hnsw;")
    op.execute("DROP INDEX IF EXISTS idx_objeto_vetor_v2_hnsw;")
    
    # Remover colunas vector
    op.execute("ALTER TABLE documento_vetores DROP COLUMN IF EXISTS objeto_vetor_v3_vector;")
    op.execute("ALTER TABLE documento_vetores DROP COLUMN IF EXISTS objeto_vetor_v2_vector;")
    
    # Nota: n√£o removemos a extens√£o vector para evitar problemas
    print("‚úÖ Rollback para FLOAT[] conclu√≠do")
```

### 3. Atualizar main.py

```python
# main.py

from pgvector.sqlalchemy import Vector
from sqlalchemy import text

# Modelo SQLAlchemy
class DocumentoVetores(Base):
    __tablename__ = "documento_vetores"
    
    id = Column(Integer, primary_key=True)
    parceria_id = Column(Integer, ForeignKey("instrumentos_parceria.id"))
    
    # Manter colunas antigas para compatibilidade
    objeto_vetor_v2 = Column(ARRAY(Float))
    objeto_vetor_v3 = Column(ARRAY(Float))
    
    # Novas colunas pgvector
    objeto_vetor_v2_vector = Column(Vector(384))
    objeto_vetor_v3_vector = Column(Vector(384))

# Endpoint de busca atualizado
@app.get("/api/v1/parcerias/semantic-busca")
async def busca_semantica(
    termo: str,
    version: str = "v3",
    limit: int = 10,
    offset: int = 0,
    db: Session = Depends(get_db)
):
    # Gerar embedding da query
    query_embedding = sentence_model.encode(termo)
    
    # Escolher coluna baseado na vers√£o
    vector_column = "objeto_vetor_v3_vector" if version == "v3" else "objeto_vetor_v2_vector"
    
    # Query usando operador pgvector <=>
    sql = text(f"""
        SELECT 
            ip.*,
            1 - (dv.{vector_column} <=> :query_vector) AS similarity_score
        FROM instrumentos_parceria ip
        JOIN documento_vetores dv ON dv.parceria_id = ip.id
        WHERE dv.{vector_column} IS NOT NULL
        ORDER BY dv.{vector_column} <=> :query_vector
        LIMIT :limit OFFSET :offset
    """)
    
    result = db.execute(sql, {
        "query_vector": query_embedding.tolist(),
        "limit": limit,
        "offset": offset
    })
    
    return {"items": result.fetchall(), "total_items": result.rowcount}
```

### 4. Tuning do √≠ndice HNSW

Ap√≥s implementar, ajustar par√¢metros baseado em m√©tricas:

```sql
-- Par√¢metro de busca (runtime, n√£o requer rebuild)
-- ef_search: quantos vizinhos explorar durante busca
-- Maior = mais preciso, mais lento
SET hnsw.ef_search = 40;  -- padr√£o: 40, range: 10-200

-- Para rebuild com novos par√¢metros:
DROP INDEX idx_objeto_vetor_v3_hnsw;
CREATE INDEX idx_objeto_vetor_v3_hnsw 
ON documento_vetores 
USING hnsw (objeto_vetor_v3_vector vector_cosine_ops)
WITH (
    m = 24,              -- mais conex√µes = melhor recall, mais espa√ßo
    ef_construction = 100 -- maior = melhor qualidade, constru√ß√£o mais lenta
);
```

### 5. Benchmarking

Script para comparar performance antes/depois:

```python
# scripts/benchmark_pgvector.py

import time
import psycopg2
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

queries = [
    "educa√ß√£o infantil",
    "sa√∫de p√∫blica",
    "assist√™ncia social",
    "cultura e arte",
    "esporte e lazer"
]

conn = psycopg2.connect(
    "host=localhost port=5433 dbname=hub_aura_db user=postgres password=rx1800"
)

for query in queries:
    embedding = model.encode(query).tolist()
    
    # Testar com scan completo (FLOAT[])
    start = time.time()
    cur = conn.cursor()
    cur.execute("""
        WITH cosine_similarity AS (
            SELECT parceria_id,
                   (SELECT SUM(a*b) FROM unnest(objeto_vetor_v3, %s) AS t(a,b)) /
                   ((SELECT sqrt(SUM(a*a)) FROM unnest(objeto_vetor_v3) AS a) *
                    (SELECT sqrt(SUM(b*b)) FROM unnest(%s) AS b)) AS similarity
            FROM documento_vetores
            WHERE objeto_vetor_v3 IS NOT NULL
        )
        SELECT * FROM cosine_similarity ORDER BY similarity DESC LIMIT 10;
    """, (embedding, embedding))
    results_scan = cur.fetchall()
    time_scan = (time.time() - start) * 1000
    
    # Testar com HNSW (pgvector)
    start = time.time()
    cur.execute("""
        SELECT parceria_id,
               1 - (objeto_vetor_v3_vector <=> %s::vector) AS similarity
        FROM documento_vetores
        WHERE objeto_vetor_v3_vector IS NOT NULL
        ORDER BY objeto_vetor_v3_vector <=> %s::vector
        LIMIT 10;
    """, (embedding, embedding))
    results_hnsw = cur.fetchall()
    time_hnsw = (time.time() - start) * 1000
    
    print(f"\nQuery: '{query}'")
    print(f"  Scan completo: {time_scan:.1f}ms")
    print(f"  HNSW:         {time_hnsw:.1f}ms")
    print(f"  Speedup:      {time_scan/time_hnsw:.1f}x")
    
    # Calcular recall (quantos resultados batem)
    ids_scan = set([r[0] for r in results_scan])
    ids_hnsw = set([r[0] for r in results_hnsw])
    recall = len(ids_scan & ids_hnsw) / len(ids_scan) * 100
    print(f"  Recall:       {recall:.1f}%")

conn.close()
```

## M√©tricas de Decis√£o

### Implementar quando:
- ‚úÖ Tempo m√©dio de busca > 500ms consistentemente
- ‚úÖ Dataset > 5.000 registros
- ‚úÖ Recall de 95% √© aceit√°vel (vs 100% atual)
- ‚úÖ Equipe tem capacidade de debug/tuning

### N√£o implementar se:
- ‚ùå Dataset < 5.000 registros
- ‚ùå Performance atual < 300ms
- ‚ùå Precis√£o de 100% √© requisito legal/regulat√≥rio
- ‚ùå Equipe sem experi√™ncia com pgvector

## Custos da Migra√ß√£o

### Tempo de implementa√ß√£o:
- Instala√ß√£o pgvector: 30 min
- Migration + testes: 2-4 horas
- Tuning e otimiza√ß√£o: 2-4 horas
- **Total:** ~1 dia de trabalho

### Riscos:
- ‚ö†Ô∏è Perda de 1-5% de recall
- ‚ö†Ô∏è Complexidade adicional (troubleshooting)
- ‚ö†Ô∏è Downtime durante migration (~5-10 min)

### Benef√≠cios:
- ‚úÖ 10-50x speedup em datasets grandes
- ‚úÖ Escalabilidade garantida
- ‚úÖ Uso de tecnologia padr√£o da ind√∫stria

## Alternativas

Se performance se tornar problema ANTES de implementar pgvector:

1. **Cache de resultados frequentes** (Redis/Memcached)
2. **Pr√©-computar top-K similaridades** (tabela similaridades)
3. **Sharding por categoria** (se dados permitirem)
4. **Filtros pr√©-busca** (reduzir espa√ßo de busca)

## Conclus√£o

**Para seu caso atual (276 registros):**
- ‚ùå **N√ÉO implementar pgvector/HNSW agora**
- ‚úÖ **Manter scan completo atual**
- üìä **Monitorar performance conforme dataset cresce**
- üîÑ **Revisar decis√£o quando atingir 5.000 registros**

---

**√öltima revis√£o:** 30/10/2025  
**Pr√≥xima revis√£o:** Quando dataset > 2.500 registros
